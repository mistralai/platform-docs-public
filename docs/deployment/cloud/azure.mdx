---
id: azure
title: Azure AI
sidebar_position: 3.21
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';


The Mistral AI open and commercial models can be deployed on your Azure subscription.

This page explains how to easily get started with Mistral Large deployed as an Azure AI endpoint.
If you use the Mistral AI Python client, it should be a drop-in replacement where you only need
to change the client parameters (endpoint URL, API key, model name).

## Deploying Mistral Small and Large 

Mistral AI models can be deployed on Azure AI either as:

- _pay-as-you-go managed services_ billed on endpoint usage,
- _real-time endpoints_ with quota-based billing indexed on the infrastructure you choose (only for existing open-weight models).

To deploy Mistral Small or Large as a pay-as-you-go managed service, follow the instructions from
[the Azure AI documentation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-mistral)
and select the model that your endpoint should serve.

## Querying the model

Once your model is deployed and provided that you have the relevant permissions, consuming it 
will basically be the same process as for a Mistral AI platform endpoint.

To run the examples below, you will need to define the following environment variables:
    - `AZUREAI_ENDPOINT` is your endpoint URL, should be of the form `https://your-endpoint.inference.ai.azure.com/v1/chat/completions`.
    - `AZUREAI_API_KEY` is your authentication key.

<Tabs>
    <TabItem value="curl" label="curl" default>
        ```shell
        curl --location $AZUREAI_ENDPOINT/v1/chat/completions \
            --header  'Content-Type: application/json' \
            --header 'Authorization: Bearer $AZUREAI_API_KEY' \
            --data '{
          "model": "azureai",
          "messages": [
            {
              "role": "user",
              "content": "What is the best French cheese ?"
            }
          ]
        }'
        ```
        
    </TabItem>
    <TabItem value="python" label="Python">

    You will need to install the Mistral AI Python client, by following the instructions from [the repository](https://github.com/mistralai/client-python).

    ```python
    import os
    from mistralai.client import MistralClient
    from mistralai.models.chat_completion import ChatMessage

    endpoint = os.environ["AZUREAI_ENDPOINT"]
    api_key = os.environ["AZUREAI_API_KEY"]
    model = "azureai"

    client = MistralClient(api_key=api_key,
                           endpoint=endpoint)

    # With streaming 
    for chunk in client.chat_stream(
        model=model,
        messages=[ChatMessage(role="user", content="What is the best French cheese?")],
    ):
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
    ```
        
    </TabItem>
</Tabs>



## Going further

For other usage examples, you can also check the following notebooks:

- [Basic CLI with `curl` and Python web request](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/webrequests.ipynb)
- [Mistral AI Python client example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/mistralai.ipynb)
- [Langchain example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/langchain.ipynb)
- [LiteLLM example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/litellm.ipynb)
- [OpenAI SDK example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/openaisdk.ipynb)
