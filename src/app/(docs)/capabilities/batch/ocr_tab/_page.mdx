import { SectionTab } from '@/components/layout/section-tab';
import { Tabs, TabItem } from '@/components/common/multi-codeblock';

A batch file for OCR, with `v1/ocr` as the Endpoint and `mistral-ocr-latest` as the Model, would look like the following:

<Tabs>
    <TabItem value="file" label="Batch File" default>

```bash
{"custom_id": "0", "body": {"document": {"document_url": "https://arxiv.org/pdf/2410.07073"}}}
{"custom_id": "1", "body": {"document": {"document_url": "https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/receipt.png"}}}
{"custom_id": "2", "body": {"document": {"image_url": "data:image/jpeg;base64,<base64_image>"}}}
```

    </TabItem>
    <TabItem value="output" label="Result File">

```bash
{"id":"batch-a6054116-1-d32adde3-3484-4008-ba51-2e2b9e52f420","custom_id":"0","response":{"status_code":200,"body":{"pages":[{"index":0,"markdown":"# Pixtral 12B \n\n![img-0.jpeg](img-0.jpeg)\n\n## Abstract\n\nWe introduce Pixtral 12B, a 12-billion-parameter multimodal language model. Pixtral 12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cu...we endeavour to recover the reported performance of all models by tuning evaluation settings towards individual models. We highlight that Pixtral 12B, like strong closed-source models (e.g. Gemini-1.5-Flash 8B [18] and Claude-3 Haiku [1]) is able reports strong performance without such interventions.","images":[],"dimensions":{"dpi":200,"height":2200,"width":1700}}],"model":"mistral-ocr-2505-completion","usage_info":{"pages_processed":24,"doc_size_bytes":12640953},"document_annotation":null}},"error":null}
{"id":"batch-a6054116-2-cf0ea563-dcdc-4e9e-b39f-39ba002160ec","custom_id":"1","response":{"status_code":200,"body":{"pages":[{"index":0,"markdown":"PLACE FACE UP ON DASH\nCITY OF PALO ALTO\nNOT VALID FOR\nONSTREET PARKING\n\nExpiration Date/Time\n11:59 PM\nAUG 19, 2024\n\nPurchase Date/Time: 01:34pm Aug 19, 2024\nTotal Due: $15.00\nTotal Paid: $15.00\nTicket #: 00005883\nS/N #: 520117260957\nSetting: Permit Machines\nMach Name: Civic Center\n\n#****-1224, Visa\nDISPLAY FACE UP ON DASH\n\nPERMIT EXPIRES\nAT ...KING\n\nExpiration Date/Time\n11:59 PM\nAUG 19, 2024\n\nPurchase Date/Time: 01:34pm Aug 19, 2024\nTotal Due: $15.00\nTotal Paid: $15.00\nTicket #: 00005883\nS/N #: 520117260957\nSetting: Permit Machines\nMach Name: Civic Center\n\n#****-1224, Visa\nDISPLAY FACE UP ON DASH\n\nPERMIT EXPIRES\nAT MIDNIGHT","images":[],"dimensions":{"dpi":200,"height":3210,"width":1806}}],"model":"mistral-ocr-2505-completion","usage_info":{"pages_processed":1,"doc_size_bytes":3110191},"document_annotation":null}},"error":null}
{"id":"batch-a6054116-3-1a0ba35b-9646-4613-9327-d499c0804e4b","custom_id":"2","response":{"status_code":200,"body":{"pages":[{"index":0,"markdown":"We introduce Mistral 7B, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference... a model fine-tuned to follow instructions, Mistral 7B - Instruct, that surpasses Llama 2 13B - chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src\nWebpage: https://mistral.ai/news/announcing-mistral-7b/","images":[],"dimensions":{"dpi":200,"height":490,"width":1200}}],"model":"mistral-ocr-2505-completion","usage_info":{"pages_processed":1,"doc_size_bytes":161787},"document_annotation":null}},"error":null}
```

    </TabItem>
</Tabs>

As a JSONL file, each line represents a request to the API Endpoint and Model.

<SectionTab as="h2" variant="secondary" sectionId="ocr-explained">Explanation</SectionTab>

The body request will follow the same format as the endpoint you want to run your batching, except the model id that will be provided only during the job creation to start the batch run. Below we provide an example of row with OCR:

<Tabs>
    <TabItem value="file" label="Sample Example" default>

```py
{
    "custom_id": "2", # An ID as metadata that will be returned in the output file to identify the request
    "body": { # The body of the request
        "document": { # The document to be processed
            "image_url": "data:image/jpeg;base64,<base64_image>" # Here, we are using a base64 encoded image, but you can also use a URL to a file, and document_url for pdf or other kinds of documents
        }
    }
}
```

    </TabItem>
    <TabItem value="output" label="Sample Result">

```py
{
  "id": "batch-a6054116-3-1a0ba35b-9646-4613-9327-d499c0804e4b", # The ID of the request
  "custom_id": "2", # The custom ID metadata provided in the input file
  "response": { # The response body of the request
    "status_code": 200,
    "body": {
      "pages": [
        {
          "index": 0,
          "markdown": "We introduce Mistral 7B, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference... a model fine-tuned to follow instructions, Mistral 7B - Instruct, that surpasses Llama 2 13B - chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src\nWebpage: https://mistral.ai/news/announcing-mistral-7b/",
          "images": [],
          "dimensions": {
            "dpi": 200,
            "height": 490,
            "width": 1200
          }
        }
      ],
      "model": "mistral-ocr-2505-completion",
      "usage_info": {
        "pages_processed": 1,
        "doc_size_bytes": 161787
      },
      "document_annotation": null
    }
  },
  "error": null
}
```
    </TabItem>
</Tabs>

For more information regarding OCR, visit the [Document AI - OCR](document_ai/basic_ocr) docs and the corresponding [API Spec](../api/endpoint/ocr#operation-ocr_v1_ocr_post).
