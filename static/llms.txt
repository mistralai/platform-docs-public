# MistralAI

## Docs

[Agents & Conversations](https://docs.mistral.ai/docs/agents/agents_and_conversations.md): Agents, Conversations, and Entries enhance API interactions with tools, history, and flexible event representation
[Agents Function Calling](https://docs.mistral.ai/docs/agents/agents_function_calling.md): Agents use function calling to execute tools and workflows, with built-in connectors and custom JSON schema support
[Agents Introduction](https://docs.mistral.ai/docs/agents/agents_introduction.md): AI agents are autonomous systems powered by LLMs that plan, use tools, and execute tasks to achieve goals, with APIs for multimodal models, persistent state, and collaboration
[Code Interpreter](https://docs.mistral.ai/docs/agents/connectors/code_interpreter.md): Code Interpreter enables safe, on-demand code execution in isolated containers for data analysis, graphing, and more
[Connectors Overview](https://docs.mistral.ai/docs/agents/connectors/connectors_overview.md): Connectors enable Agents and users to access tools like websearch, code interpreter, and more for on-demand answers
[Document Library](https://docs.mistral.ai/docs/agents/connectors/document_library.md): Document Library is a built-in RAG tool for agents to access and manage uploaded documents in Mistral Cloud
[Image Generation](https://docs.mistral.ai/docs/agents/connectors/image_generation.md): Built-in tool for agents to generate images on demand
[Websearch](https://docs.mistral.ai/docs/agents/connectors/websearch.md): Websearch enables models to browse the web for real-time info, overcoming outdated training data limitations
[Agents Handoffs](https://docs.mistral.ai/docs/agents/handoffs.md): Agents Handoffs enable seamless task delegation and conversation transfers between multiple agents in automated workflows
[MCP](https://docs.mistral.ai/docs/agents/mcp.md): MCP standardizes AI model integration with data sources for seamless, secure, and efficient contextual connections
[Audio & Transcription](https://docs.mistral.ai/docs/capabilities/audio_and_transcription.md): Audio & Transcription: Models for real-time chat and efficient transcription via audio input
[Batch Inference](https://docs.mistral.ai/docs/capabilities/batch_inference.md): Prepare and upload JSONL batch files, then create a job with model, endpoint, and optional metadata
[Citations and References](https://docs.mistral.ai/docs/capabilities/citations_and_references.md): Citations and references enable models to ground responses with sources, enhancing accuracy for RAG and agentic applications
[Coding](https://docs.mistral.ai/docs/capabilities/coding.md): LLMs for coding: Codestral for code generation, Devstral for agentic tool use, and Codestral Embed for semantic search
[Annotations](https://docs.mistral.ai/docs/capabilities/document_ai/annotations.md): Mistral Document AI API adds structured JSON annotations for OCR, including bbox and document annotations for efficient data extraction
[Basic OCR](https://docs.mistral.ai/docs/capabilities/document_ai/basic_ocr.md): Extract text, structure, and images from PDFs and images with Mistral's OCR API." (99 characters)
[Document AI](https://docs.mistral.ai/docs/capabilities/document_ai/document_ai_overview.md): Mistral Document AI offers enterprise-level OCR and structured data extraction with multilingual support and scalable workflows
[Document QnA](https://docs.mistral.ai/docs/capabilities/document_ai/document_qna.md): Document AI QnA combines OCR and LLMs for natural language interaction with document content, enabling question answering and insights
[Code Embeddings](https://docs.mistral.ai/docs/capabilities/embeddings/code_embeddings.md): Code embeddings enable advanced code analysis, retrieval, and AI assistant capabilities for enterprise applications
[Embeddings Overview](https://docs.mistral.ai/docs/capabilities/embeddings/embeddings_overview.md): Vector representations of text and code for NLP tasks like search, clustering, and classification
[Text Embeddings](https://docs.mistral.ai/docs/capabilities/embeddings/text_embeddings.md): Generate 1024-dimension text embeddings using Mistral AI's embeddings API for NLP applications
[Classifier Factory](https://docs.mistral.ai/docs/capabilities/finetuning/classifier-factory.md): Classifier Factory: Tools for moderation, intent detection, and sentiment analysis to enhance efficiency and user experience
[Fine-tuning Overview](https://docs.mistral.ai/docs/capabilities/finetuning/finetuning_overview.md): Learn about fine-tuning costs, benefits vs. prompting, and when to use each for AI model optimization
[Text & Vision Fine-tuning](https://docs.mistral.ai/docs/capabilities/finetuning/text-vision-finetuning.md): Fine-tune text and vision models for domain-specific tasks or conversational styles using JSONL datasets
[Function calling](https://docs.mistral.ai/docs/capabilities/function-calling.md): Mistral models enable function calling to integrate external tools for enhanced application development
[Moderation](https://docs.mistral.ai/docs/capabilities/moderation.md): New moderation API with Mistral model for detecting harmful text in raw and conversational content
[Predicted outputs](https://docs.mistral.ai/docs/capabilities/predicted-outputs.md): Optimizes response time by predefining predictable content for faster, high-quality outputs." (99 characters)
[Reasoning](https://docs.mistral.ai/docs/capabilities/reasoning.md): Reasoning enhances CoT by generating logical steps before final answers, improving problem-solving through deeper exploration
[Custom Structured Output](https://docs.mistral.ai/docs/capabilities/structured-output/custom.md): Define JSON schemas for consistent structured outputs using Pydantic and Mistral AI
[JSON mode](https://docs.mistral.ai/docs/capabilities/structured-output/json-mode.md): Enable JSON mode in API responses by setting `response_format` to `{\"type\": \"json_object\"}`. (100 characters)
[Structured Output](https://docs.mistral.ai/docs/capabilities/structured-output/overview.md): Learn to generate structured outputs like JSON or custom formats for reliable LLM agent workflows
[Text and Chat Completions](https://docs.mistral.ai/docs/capabilities/text_and_chat_completions.md): Mistral models enable chat and text completions via natural language prompts, with flexible API options for streaming and async responses
[Vision](https://docs.mistral.ai/docs/capabilities/vision.md): Vision capabilities enable models to analyze images and text for multimodal insights, with support for URL or Base64 image inputs
[AWS Bedrock](https://docs.mistral.ai/docs/deployment/cloud/aws.md): Deploy Mistral AI models on AWS Bedrock as fully managed, serverless endpoints
[Azure AI](https://docs.mistral.ai/docs/deployment/cloud/azure.md): Deploy Mistral AI models on Azure AI with pay-as-you-go or real-time GPU-based endpoints
[IBM watsonx.ai](https://docs.mistral.ai/docs/deployment/cloud/ibm-watsonx.md): Mistral AI's Large model on IBM watsonx.ai: SaaS & on-premise deployment with setup & query guidance
[Outscale](https://docs.mistral.ai/docs/deployment/cloud/outscale.md): Deploy and query Mistral AI models on Outscale via managed VMs and GPUs
[Cloud](https://docs.mistral.ai/docs/deployment/cloud/overview.md): Access Mistral AI models via Azure, AWS, Google Cloud, Snowflake, IBM, and Outscale using cloud credits
[Snowflake Cortex](https://docs.mistral.ai/docs/deployment/cloud/sfcortex.md): Access Mistral AI models on Snowflake Cortex as serverless, fully managed endpoints
[Vertex AI](https://docs.mistral.ai/docs/deployment/cloud/vertex.md): Deploy and query Mistral AI models on Google Cloud's serverless Vertex AI platform
[Workspaces](https://docs.mistral.ai/docs/deployment/laplateforme/organization.md): La Plateforme workspaces enable team collaboration, access control, and shared fine-tuned models." (99 characters)
[La Plateforme](https://docs.mistral.ai/docs/deployment/laplateforme/overview.md): Mistral AI's pay-as-you-go API platform for accessing latest models via [La Plateforme][platform_url]
[Pricing](https://docs.mistral.ai/docs/deployment/laplateforme/pricing.md): Check the pricing page for detailed API cost information
[Rate limit and usage tiers](https://docs.mistral.ai/docs/deployment/laplateforme/tier.md): Learn about Mistral's API rate limits, usage tiers, and how to check or adjust your workspace limits
[Deploy with Cerebrium](https://docs.mistral.ai/docs/deployment/self-deployment/cerebrium.md): Deploy AI apps effortlessly with Cerebrium's serverless GPU infrastructure, auto-scaling and pay-per-use
[Deploy with Cloudflare Workers AI](https://docs.mistral.ai/docs/deployment/self-deployment/cloudflare.md): Deploy AI models on Cloudflare's global network with Workers AI for serverless GPU-powered LLMs
[Self-deployment](https://docs.mistral.ai/docs/deployment/self-deployment/overview.md): Deploy Mistral AI models on your infrastructure using vLLM, TensorRT-LLM, TGI, or tools like SkyPilot and Cerebrium
[Deploy with SkyPilot](https://docs.mistral.ai/docs/deployment/self-deployment/skypilot.md): Deploy AI models on any cloud with SkyPilot for cost savings and high GPU availability
[Text Generation Inference](https://docs.mistral.ai/docs/deployment/self-deployment/tgi.md): TGI is a high-performance toolkit for deploying and serving open-access LLMs with features like quantization and streaming
[TensorRT](https://docs.mistral.ai/docs/deployment/self-deployment/trt.md): Guide to building and deploying TensorRT-LLM engines for Mistral and Mixtral models using Triton Inference Server
[vLLM](https://docs.mistral.ai/docs/deployment/self-deployment/vllm.md): vLLM is an open-source LLM inference engine optimized for deploying Mistral models on-premise
[SDK Clients](https://docs.mistral.ai/docs/getting-started/clients.md): SDK clients available in Python, Typescript, and community-supported languages
[Bienvenue to Mistral AI Documentation](https://docs.mistral.ai/docs/getting-started/docs_introduction.md): Mistral AI offers open-source and commercial LLMs for developers, featuring multilingual, coding, and reasoning capabilities
[Glossary](https://docs.mistral.ai/docs/getting-started/glossary.md): Glossary of key terms related to large language models (LLMs), text generation, and tokens
[Model customization](https://docs.mistral.ai/docs/getting-started/model_customization.md): Learn how to build applications with custom LLMs for iterative, user-driven AI development
[Models Benchmarks](https://docs.mistral.ai/docs/getting-started/models/benchmark.md): Standardized tests evaluating LLM performance, including Mistral's top-tier reasoning and multilingual benchmarks
[Model selection](https://docs.mistral.ai/docs/getting-started/models/model_selection.md): Guide to selecting Mistral models based on task complexity, performance, and cost for optimal use cases
[Models Overview](https://docs.mistral.ai/docs/getting-started/models/overview.md): Mistral offers open and premier models, including multimodal and reasoning options with API access and commercial licensing
[Model weights](https://docs.mistral.ai/docs/getting-started/models/weights.md): Open-source pre-trained and instruction-tuned models under various licenses, with download links and usage guidelines
[Quickstart](https://docs.mistral.ai/docs/getting-started/quickstart.md): Quickstart guide to setting up a Mistral account, configuring billing, and generating API keys for Mistral AI
[Basic RAG](https://docs.mistral.ai/docs/guides/basic-RAG.md): RAG combines LLMs with retrieval systems to generate answers using external knowledge, involving retrieval and generation steps
[Ambassador](https://docs.mistral.ai/docs/guides/contribute/ambassador.md): Join Mistral AI's Ambassador Program to advocate for AI, share expertise, and support the community. Apply by July 1, 2025
[Contribute](https://docs.mistral.ai/docs/guides/contribute/overview.md): Learn how to contribute to Mistral AI through docs, code, or the Ambassador Program
[Evaluation](https://docs.mistral.ai/docs/guides/evaluation.md): Guide to evaluating LLMs for specific use cases with metrics, LLM, and human-based methods
[Fine-tuning](https://docs.mistral.ai/docs/guides/finetuning.md): Fine-tuning models incurs a $2 monthly storage fee; see pricing for details
[ 01 Intro Basics](https://docs.mistral.ai/docs/guides/finetuning_sections/_01_intro_basics.md): Learn the basics of fine-tuning LLMs to optimize performance for specific tasks using Mistral AI's API and open-source tools
[ 02 Prepare Dataset](https://docs.mistral.ai/docs/guides/finetuning_sections/_02_prepare_dataset.md): Prepare training data for fine-tuning models with specific use cases and examples." (99 characters)
[download the validation and reformat script](https://docs.mistral.ai/docs/guides/finetuning_sections/_03_e2e_examples.md): Download and use the reformat_data.py script to validate and reformat Mistral API fine-tuning datasets
[get data from hugging face](https://docs.mistral.ai/docs/guides/finetuning_sections/_04_faq.md): Learn how to fetch, validate, and format data from Hugging Face for Mistral models
[Observability](https://docs.mistral.ai/docs/guides/observability.md): Observability ensures visibility, debugging, and continuous improvement for LLM systems in production." (99 characters)
[Other resources](https://docs.mistral.ai/docs/guides/other-resources.md): Explore Mistral AI Cookbook for code examples, community contributions, and third-party tool integrations
[Prefix](https://docs.mistral.ai/docs/guides/prefix.md): Prefixes enhance instruction following and response control for models in various use cases
[Prompting capabilities](https://docs.mistral.ai/docs/guides/prompting-capabilities.md): Learn how to craft effective prompts for classification, summarization, personalization, and evaluation with Mistral models
[Sampling](https://docs.mistral.ai/docs/guides/sampling.md): Learn how to adjust LLM sampling parameters like Temperature, Top P, and penalties for better output control
[Tokenization](https://docs.mistral.ai/docs/guides/tokenization.md): Tokenization breaks text into tokens for LLM processing, with Mistral AI's open-source tools for Python implementation