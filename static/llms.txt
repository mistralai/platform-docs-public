# MistralAI

## Docs

[Agents & Conversations](https://docs.mistral.ai/docs/agents/agents_and_conversations.md): Agents, Conversations, and Entries enhance API interactions with tools, history, and flexible event representation
[Agents Function Calling](https://docs.mistral.ai/docs/agents/agents_function_calling.md): Agents use function calling to execute tools and workflows, with built-in connectors and custom JSON schema support
[Agents Introduction](https://docs.mistral.ai/docs/agents/agents_introduction.md): AI agents are autonomous systems powered by LLMs that plan, use tools, and execute tasks to achieve goals, with APIs for multimodal models, persistent state, and collaboration
[Code Interpreter](https://docs.mistral.ai/docs/agents/connectors/code_interpreter.md): Code Interpreter enables secure, on-demand code execution in isolated containers for data analysis, graphing, and more
[Connectors Overview](https://docs.mistral.ai/docs/agents/connectors/connectors_overview.md): Connectors enable Agents and users to access tools like websearch, code interpreter, and more for on-demand answers
[Document Library](https://docs.mistral.ai/docs/agents/connectors/document_library.md): Document Library is a built-in RAG tool for agents to access and manage uploaded documents in Mistral Cloud
[Image Generation](https://docs.mistral.ai/docs/agents/connectors/image_generation.md): Image Generation tool enables agents to create images on demand." (99 characters)
[Websearch](https://docs.mistral.ai/docs/agents/connectors/websearch.md): Websearch enables models to browse the web for real-time info, bypassing training data limitations with search and URL access
[Agents Handoffs](https://docs.mistral.ai/docs/agents/handoffs.md): Agents Handoffs enable seamless task delegation and conversation transfers between multiple agents in automated workflows
[MCP](https://docs.mistral.ai/docs/agents/mcp.md): MCP standardizes AI model integration with data sources for seamless, secure, and efficient contextual access
[Audio & Transcription](https://docs.mistral.ai/docs/capabilities/audio_and_transcription.md): Audio & Transcription: Models for chat and transcription with audio input support
[Batch Inference](https://docs.mistral.ai/docs/capabilities/batch_inference.md): Prepare and upload batch requests, then create a job to process them with specified models and endpoints
[Citations and References](https://docs.mistral.ai/docs/capabilities/citations_and_references.md): Citations and references enable models to ground responses with sources, enhancing RAG and agentic applications
[Coding](https://docs.mistral.ai/docs/capabilities/coding.md): LLMs for coding: Codestral for code generation, Devstral for agentic tool use, with FIM and chat endpoints
[Annotations](https://docs.mistral.ai/docs/capabilities/document_ai/annotations.md): Mistral Document AI API adds structured JSON annotations for OCR, including bbox and document annotations for efficient data extraction
[Basic OCR](https://docs.mistral.ai/docs/capabilities/document_ai/basic_ocr.md): Extract text and structured content from PDFs with Mistral's OCR API, preserving formatting and supporting multiple formats
[Document AI](https://docs.mistral.ai/docs/capabilities/document_ai/document_ai_overview.md): Mistral Document AI offers enterprise-level OCR, structured data extraction, and multilingual support for fast, accurate document processing
[Document QnA](https://docs.mistral.ai/docs/capabilities/document_ai/document_qna.md): Document AI QnA enables natural language queries on documents using OCR and large language models for insights and answers
[Code Embeddings](https://docs.mistral.ai/docs/capabilities/embeddings/code_embeddings.md): Code embeddings power retrieval, clustering, and analytics for code databases and coding assistants
[Embeddings Overview](https://docs.mistral.ai/docs/capabilities/embeddings/embeddings_overview.md): Mistral AI's Embeddings API provides state-of-the-art vector representations for text and code, enabling NLP tasks like retrieval, clustering, and search
[Text Embeddings](https://docs.mistral.ai/docs/capabilities/embeddings/text_embeddings.md): Generate 1024-dimension text embeddings using Mistral AI's embeddings API for NLP applications
[Classifier Factory](https://docs.mistral.ai/docs/capabilities/finetuning/classifier-factory.md): Classifier Factory: Tools for moderation, intent detection, and sentiment analysis to enhance efficiency and user experience
[Fine-tuning Overview](https://docs.mistral.ai/docs/capabilities/finetuning/finetuning_overview.md): Learn about fine-tuning costs, storage fees, and when to choose it over prompt engineering for AI models
[Text & Vision Fine-tuning](https://docs.mistral.ai/docs/capabilities/finetuning/text-vision-finetuning.md): Fine-tune text and vision models for domain-specific tasks or conversational styles using JSONL datasets
[Function calling](https://docs.mistral.ai/docs/capabilities/function-calling.md): Mistral models enable function calling to integrate external tools for custom applications and problem-solving
[Moderation](https://docs.mistral.ai/docs/capabilities/moderation.md): New moderation API using Mistral model to detect harmful text in raw and conversational content
[Predicted outputs](https://docs.mistral.ai/docs/capabilities/predicted-outputs.md): Optimizes response time by predefining predictable content to improve efficiency in tasks like code editing
[Reasoning](https://docs.mistral.ai/docs/capabilities/reasoning.md): Reasoning enhances CoT by generating logical steps before conclusions, improving problem-solving with deeper exploration
[Custom Structured Output](https://docs.mistral.ai/docs/capabilities/structured-output/custom.md): Define and enforce JSON output structure using Pydantic models with Mistral AI
[JSON mode](https://docs.mistral.ai/docs/capabilities/structured-output/json-mode.md): Enable JSON mode by setting `response_format` to `{\"type\": \"json_object\"}` in API requests
[Structured Output](https://docs.mistral.ai/docs/capabilities/structured-output/overview.md): Learn to generate structured JSON or custom outputs from LLMs for reliable agent workflows
[Text and Chat Completions](https://docs.mistral.ai/docs/capabilities/text_and_chat_completions.md): Mistral models enable chat and text completions via natural language prompts, with flexible API options for streaming and async responses
[Vision](https://docs.mistral.ai/docs/capabilities/vision.md): Vision models analyze images and text for multimodal insights, supporting applications like document parsing and data extraction
[AWS Bedrock](https://docs.mistral.ai/docs/deployment/cloud/aws.md): Deploy Mistral AI models on AWS Bedrock as fully managed, serverless endpoints
[Azure AI](https://docs.mistral.ai/docs/deployment/cloud/azure.md): Deploy Mistral AI models on Azure AI with pay-as-you-go or real-time GPU-based endpoints
[IBM watsonx.ai](https://docs.mistral.ai/docs/deployment/cloud/ibm-watsonx.md): Mistral AI's Large model on IBM watsonx.ai for managed & on-premise deployments with API access setup
[Outscale](https://docs.mistral.ai/docs/deployment/cloud/outscale.md): Deploy and query Mistral AI models on Outscale via managed VMs and GPUs
[Cloud](https://docs.mistral.ai/docs/deployment/cloud/overview.md): Access Mistral AI models via Azure, AWS, Google Cloud, Snowflake, IBM, and Outscale using cloud credits
[Snowflake Cortex](https://docs.mistral.ai/docs/deployment/cloud/sfcortex.md): Access Mistral AI models on Snowflake Cortex as serverless, fully managed endpoints
[Vertex AI](https://docs.mistral.ai/docs/deployment/cloud/vertex.md): Deploy Mistral AI models on Google Cloud Vertex AI as serverless endpoints
[Workspaces](https://docs.mistral.ai/docs/deployment/laplateforme/organization.md): La Plateforme workspaces enable team collaboration, access management, and shared fine-tuned models." (99 characters)
[La Plateforme](https://docs.mistral.ai/docs/deployment/laplateforme/overview.md): Mistral AI's pay-as-you-go API platform for accessing latest large language models
[Pricing](https://docs.mistral.ai/docs/deployment/laplateforme/pricing.md): Check the pricing page for detailed API cost information." (99 characters)
[Rate limit and usage tiers](https://docs.mistral.ai/docs/deployment/laplateforme/tier.md): Learn about Mistral's API rate limits, usage tiers, and how to check or upgrade your workspace limits
[Deploy with Cerebrium](https://docs.mistral.ai/docs/deployment/self-deployment/cerebrium.md): Deploy AI apps effortlessly with Cerebrium's serverless GPU infrastructure, auto-scaling and pay-per-use
[Deploy with Cloudflare Workers AI](https://docs.mistral.ai/docs/deployment/self-deployment/cloudflare.md): Deploy AI models on Cloudflare's global network with serverless GPUs via Workers AI
[Self-deployment](https://docs.mistral.ai/docs/deployment/self-deployment/overview.md): Deploy Mistral AI models on your infrastructure using vLLM, TensorRT-LLM, TGI, or tools like SkyPilot and Cerebrium
[Deploy with SkyPilot](https://docs.mistral.ai/docs/deployment/self-deployment/skypilot.md): Deploy AI models on any cloud with SkyPilot for cost savings, high GPU availability, and managed execution
[Text Generation Inference](https://docs.mistral.ai/docs/deployment/self-deployment/tgi.md): TGI is a high-performance toolkit for deploying and serving open-access LLMs with features like quantization and streaming
[TensorRT](https://docs.mistral.ai/docs/deployment/self-deployment/trt.md): Guide to building and deploying TensorRT-LLM engines for Mistral-7B and Mixtral-8X7B models
[vLLM](https://docs.mistral.ai/docs/deployment/self-deployment/vllm.md): vLLM is an open-source LLM inference engine optimized for deploying Mistral models on-premise
[SDK Clients](https://docs.mistral.ai/docs/getting-started/clients.md): Python & Typescript SDK clients for Mistral AI, with community third-party options
[Bienvenue to Mistral AI Documentation](https://docs.mistral.ai/docs/getting-started/docs_introduction.md): Mistral AI offers open-source and commercial LLMs for developers, with premier models like Mistral Medium and Codestral
[Glossary](https://docs.mistral.ai/docs/getting-started/glossary.md): Glossary of key terms related to large language models (LLMs) and text generation
[Model customization](https://docs.mistral.ai/docs/getting-started/model_customization.md): Guide to building applications with custom LLMs for iterative, user-driven AI development
[Models Benchmarks](https://docs.mistral.ai/docs/getting-started/models/benchmark.md): Standardized benchmarks evaluate LLM performance, comparing strengths in reasoning, multilingual tasks, math, and code generation
[Model selection](https://docs.mistral.ai/docs/getting-started/models/model_selection.md): Guide to selecting Mistral models based on performance, cost, and use-case complexity
[Models Overview](https://docs.mistral.ai/docs/getting-started/models/overview.md): Mistral offers open and premier models, including multimodal and reasoning options with API access and commercial licensing
[Model weights](https://docs.mistral.ai/docs/getting-started/models/weights.md): Open-source pre-trained and instruction-tuned models with varying licenses; commercial options available
[Quickstart](https://docs.mistral.ai/docs/getting-started/quickstart.md): Set up your Mistral account, configure billing, and generate API keys to start using Mistral AI
[Basic RAG](https://docs.mistral.ai/docs/guides/basic-RAG.md): RAG combines LLMs with retrieval systems to generate answers using external knowledge." (99 characters)
[Ambassador](https://docs.mistral.ai/docs/guides/contribute/ambassador.md): Join Mistral AI's Ambassador Program to advocate for AI, share expertise, and support the community. Apply by July 1, 2025
[Contribute](https://docs.mistral.ai/docs/guides/contribute/overview.md): Learn how to contribute to Mistral AI through docs, code, or the Ambassador Program
[Evaluation](https://docs.mistral.ai/docs/guides/evaluation.md): Guide to evaluating LLMs for specific use cases with metrics, LLM, and human-based methods
[Fine-tuning](https://docs.mistral.ai/docs/guides/finetuning.md): Fine-tuning models incurs a $2 monthly storage fee; see pricing for details
[ 01 Intro Basics](https://docs.mistral.ai/docs/guides/finetuning_sections/_01_intro_basics.md): Learn the basics of fine-tuning LLMs to optimize performance for specific tasks using Mistral AI's tools
[ 02 Prepare Dataset](https://docs.mistral.ai/docs/guides/finetuning_sections/_02_prepare_dataset.md): Prepare training data for fine-tuning models with specific use cases and examples
[download the validation and reformat script](https://docs.mistral.ai/docs/guides/finetuning_sections/_03_e2e_examples.md): Download the reformat_data.py script to validate and reformat Mistral API fine-tuning datasets
[get data from hugging face](https://docs.mistral.ai/docs/guides/finetuning_sections/_04_faq.md): Learn how to fetch, validate, and format data from Hugging Face for Mistral models
[Observability](https://docs.mistral.ai/docs/guides/observability.md): Observability ensures visibility, debugging, and continuous improvement for LLM systems in production." (99 characters)
[Other resources](https://docs.mistral.ai/docs/guides/other-resources.md): Explore Mistral AI Cookbook for code examples, community contributions, and third-party tool integrations
[Prefix](https://docs.mistral.ai/docs/guides/prefix.md): Prefixes enhance instruction adherence and response control for models in various use cases
[Prompting capabilities](https://docs.mistral.ai/docs/guides/prompting-capabilities.md): Learn how to craft effective prompts for classification, summarization, personalization, and evaluation with Mistral models
[Sampling](https://docs.mistral.ai/docs/guides/sampling.md): Learn how to adjust LLM sampling parameters like Temperature, Top P, and penalties for better output control
[Tokenization](https://docs.mistral.ai/docs/guides/tokenization.md): Tokenization breaks text into subword units for LLM processing, with Mistral AI's open-source tools for Python