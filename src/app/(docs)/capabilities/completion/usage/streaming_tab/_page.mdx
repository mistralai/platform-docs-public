import { Tabs, TabItem } from '@/components/common/multi-codeblock';

For streaming chat completions requests, you will provide a list of messages and the model will return a **stream of chunks of the completion response**.
The **latency for the first token will depend mostly on the number of input tokens** corresponding to the list of input messages, after that you will receive a **stream of tokens until the model decides to stop or the maximum number of tokens is reached**.

Note that in between streamed tokens you may get different **interleaved events**, such as [tool calls](../function_calling) and [citations](./citations).  
The content can be either a string, the most standard usage of llms:
- `{'content': '...'}`  

...or a list of different types of contents:
- `{'content': [{'type': 'text', 'text': '...'}, {'type': '...', '...': [...]}, ...]}`.

<Tabs groupId="code">
    <TabItem value="python" label="python" default>
        <Tabs groupId="syn">
            <TabItem value="sync" label="Synchronous" default>

```python
import os
from mistralai import Mistral

api_key = os.environ["MISTRAL_API_KEY"]
model = "mistral-medium-latest"

client = Mistral(api_key=api_key)

stream_response = client.chat.stream(
    model = model,
    messages = [
        {
            "role": "user",
            "content": "What is the best French cheese?",
        },
    ]
)

# If you want to print the stream text to the console
for chunk in stream_response:
    print(chunk.data.choices[0].delta.content)
```

            </TabItem>
            <TabItem value="async" label="Asynchronous">

```py
import asyncio
import os

from mistralai import Mistral

async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    model = "mistral-medium-latest"

    client = Mistral(api_key=api_key)

    response = await client.chat.stream_async(
        model=model,
        messages=[
             {
                  "role": "user",
                  "content": "Who is the best French painter? Answer in JSON.",
              },
        ],
    )

    # If you want to print the stream text to the console
    async for chunk in response:
        if chunk.data.choices[0].delta.content is not None:
            print(chunk.data.choices[0].delta.content, end="")

if __name__ == "__main__":
    asyncio.run(main())
```

            </TabItem>
        </Tabs>
    </TabItem>
    <TabItem value="typescript" label="typescript">

```typescript
import { Mistral } from "@mistralai/mistralai";
import * as dotenv from 'dotenv';

dotenv.config();

const apiKey = process.env["MISTRAL_API_KEY"];

const client = new Mistral({ apiKey: apiKey });

async function main() {

    const result = await client.chat.stream({
        model: "mistral-medium-latest",
        messages: [{ role: "user", content: "What is the best French cheese?" }],
    });

    // If you want to print the stream text to the console
    for await (const chunk of result) {
        const streamText = chunk.data.choices[0].delta.content;
        if (typeof streamText === "string") {
            process.stdout.write(streamText);
        }
    }
}

main()
```

    </TabItem>
    <TabItem value="curl" label="curl">

```bash
curl --location "https://api.mistral.ai/v1/chat/completions" \
     --header 'Content-Type: application/json' \
     --header 'Accept: application/json' \
     --header "Authorization: Bearer $MISTRAL_API_KEY" \
     --data '{
    "model": "mistral-medium-latest",
    "messages": [
     {
        "role": "user",
        "content": "What is the best French cheese?"
      }
    ],
    "stream": true
  }'
```

    </TabItem>
    <TabItem value="output" label="output">

```json
data: {
  "id":"64a7f9da74be4e06bccfcff4a2b3ed03",
  "object":"chat.completion.chunk",
  "created":1756223818,
  "model":"mistral-medium-latest",
  "choices":[
    {
      "index":0,
      "delta": {
        "role":"assistant",
        "content":""
      },
      "finish_reason":null
    }
  ]
}
data: {
  "id": "64a7f9da74be4e06bccfcff4a2b3ed03",
  "object": "chat.completion.chunk",
  "created": 1756223818,
  "model": "mistral-medium-latest",
  "choices": [
    {
      "index": 0,
      "delta": {
        "content": "The \""
      },
      "finish_reason": null
    }
  ]
}
data: {
  "id": "64a7f9da74be4e06bccfcff4a2b3ed03",
  "object": "chat.completion.chunk",
  "created": 1756223818,
  "model": "mistral-medium-latest",
  "choices": [
    {
      "index": 0,
      "delta": {
        "content": "best"
      },
      "finish_reason": null
    }
  ]
}
...
data: {
  "id": "64a7f9da74be4e06bccfcff4a2b3ed03",
  "object": "chat.completion.chunk",
  "created": 1756223818,
  "model": "mistral-medium-latest",
  "choices": [
    {
      "index": 0,
      "delta": {
        "content": " dish"
      },
      "finish_reason": null
    }
  ]
}
data: {
  "id": "64a7f9da74be4e06bccfcff4a2b3ed03",
  "object": "chat.completion.chunk",
  "created": 1756223818,
  "model": "mistral-medium-latest",
  "choices": [
    {
      "index": 0,
      "delta": {
        "content": "?"
      },
      "finish_reason": null
    }
  ]
}
data: {
  "id": "64a7f9da74be4e06bccfcff4a2b3ed03",
  "object": "chat.completion.chunk",
  "created": 1756223818,
  "model": "mistral-medium-latest",
  "choices": [
    {
      "index": 0,
      "delta": {
        "content": ""
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "total_tokens": 1254,
    "completion_tokens": 1244
  }
}
data: [DONE]
```

    </TabItem>
</Tabs>
