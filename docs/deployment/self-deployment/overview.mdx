---
id: self_deployment_overview
title: Self-deployment
slug: overview
---

Mistral AI models can be self-deployed on your own infrastructure through various
inference engines. We recommend using [vLLM](https://vllm.readthedocs.io/), a
highly-optimized Python-only serving framework which can expose an OpenAI-compatible
API.

Other inference engine alternatives include 
[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) and
[TGI](https://huggingface.co/docs/text-generation-inference/index).

You can also leverage specific tools to facilitate infrastructure management, such as 
[SkyPilot](https://skypilot.readthedocs.io) or [Cerebrium](https://www.cerebrium.ai).
