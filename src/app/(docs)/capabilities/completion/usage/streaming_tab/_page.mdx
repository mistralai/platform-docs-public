import { Tabs, TabItem } from '@/components/common/multi-codeblock';

For streaming chat completions requests, you will provide a list of messages and the model will return a **stream of chunks of the completion response**.
The **latency for the first token will depend mostly on the number of input tokens** corresponding to the list of input messages, after that you will receive a **stream of tokens until the model decides to stop or the maximum number of tokens is reached**.

Note that in between streamed tokens you may get different **interleaved events**, such as [tool calls](../function_calling) and [citations](../citations).  
The content can be either a string, the most standard usage of llms:
- `{'content': '...'}`  

...or a list of different types of contents:
- `{'content': [{'type': 'text', 'text': '...'}, {'type': '...', '...': [...]}, ...]}`.

<Tabs groupId="code">
    <TabItem value="python" label="python" default>
        <Tabs groupId="syn">
            <TabItem value="sync" label="Synchronous" default>

```python
import os
from mistralai import Mistral

api_key = os.environ["MISTRAL_API_KEY"]
model = "mistral-large-latest"

client = Mistral(api_key=api_key)

stream_response = client.chat.stream(
    model = model,
    messages = [
        {
            "role": "user",
            "content": "How far is the moon from earth? Answer with the distance in km only.",
        },
    ]
)

# If you want to print the stream text to the console
for chunk in stream_response:
    print(chunk.data.choices[0].delta.content)
```

            </TabItem>
            <TabItem value="async" label="Asynchronous">

```py
import asyncio
import os

from mistralai import Mistral

async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    model = "mistral-large-latest"

    client = Mistral(api_key=api_key)

    response = await client.chat.stream_async(
        model=model,
        messages=[
             {
                  "role": "user",
                  "content": "How far is the moon from earth? Answer with the distance in km only.",
              },
        ],
    )

    # If you want to print the stream text to the console
    async for chunk in response:
        if chunk.data.choices[0].delta.content is not None:
            print(chunk.data.choices[0].delta.content, end="")

if __name__ == "__main__":
    asyncio.run(main())
```

            </TabItem>
        </Tabs>
    </TabItem>
    <TabItem value="typescript" label="typescript">

```typescript
import { Mistral } from "@mistralai/mistralai";
import * as dotenv from 'dotenv';

dotenv.config();

const apiKey = process.env["MISTRAL_API_KEY"];

const client = new Mistral({ apiKey: apiKey });

async function main() {

    const result = await client.chat.stream({
        model: "mistral-large-latest",
        messages: [{ role: "user", content: "How far is the moon from earth? Answer with the distance in km only." }],
    });

    // If you want to print the stream text to the console
    for await (const chunk of result) {
        const streamText = chunk.data.choices[0].delta.content;
        if (typeof streamText === "string") {
            process.stdout.write(streamText);
        }
    }
}

main()
```

    </TabItem>
    <TabItem value="curl" label="curl">

```bash
curl --location "https://api.mistral.ai/v1/chat/completions" \
     --header 'Content-Type: application/json' \
     --header 'Accept: application/json' \
     --header "Authorization: Bearer $MISTRAL_API_KEY" \
     --data '{
    "model": "mistral-large-latest",
    "messages": [
     {
        "role": "user",
        "content": "How far is the moon from earth? Answer with the distance in km only."
      }
    ],
    "stream": true
  }'
```

    </TabItem>
    <TabItem value="output" label="output">

```json
{
  "id": "59060ef9339a4112b2c9e57e3ee6199d",
  "model": "mistral-large-latest",
  "choices": [
    {
      "index": 0,
      "delta": {
        "role": "assistant",
        "content": ""
      },
      "finish_reason": null
    }
  ],
  "object": "chat.completion.chunk",
  "created": 1764258570,
  "usage": null
}
{
  "id": "59060ef9339a4112b2c9e57e3ee6199d",
  "model": "mistral-large-latest",
  "choices": [
    {
      "index": 0,
      "delta": {
        "content": "3"
      },
      "finish_reason": null
    }
  ],
  "object": "chat.completion.chunk",
  "created": 1764258570,
  "usage": null
}
{
  "id": "59060ef9339a4112b2c9e57e3ee6199d",
  "model": "mistral-large-latest",
  "choices": [
    {
      "index": 0,
      "delta": {
        "content": "84"
      },
      "finish_reason": null
    }
  ],
  "object": "chat.completion.chunk",
  "created": 1764258570,
  "usage": null
}
{
  "id": "59060ef9339a4112b2c9e57e3ee6199d",
  "model": "mistral-large-latest",
  "choices": [
    {
      "index": 0,
      "delta": {
        "content": "4"
      },
      "finish_reason": null
    }
  ],
  "object": "chat.completion.chunk",
  "created": 1764258570,
  "usage": null
}
{
  "id": "59060ef9339a4112b2c9e57e3ee6199d",
  "model": "mistral-large-latest",
  "choices": [
    {
      "index": 0,
      "delta": {
        "content": "00"
      },
      "finish_reason": "stop"
    }
  ],
  "object": "chat.completion.chunk",
  "created": 1764258570,
  "usage": {
    "prompt_tokens": 19,
    "completion_tokens": 7,
    "total_tokens": 26
  }
}
```
    </TabItem>
</Tabs>
