---
id: annotations 
title: Annotations
slug: annotations
sidebar_position: 3.2
---

import { Tabs, TabItem } from '@/components/common/multi-codeblock';
import { SectionTab } from '@/components/layout/section-tab';
import { ExplorerTabs, ExplorerTab } from '@/components/common/explorer-tabs';
import { Faq, FaqItem } from '@/components/common/faq';

import BBoxAnnotationTab from './bbox_annotation_tab/_page.mdx';
import DocumentAnnotationTab from './document_annotation_tab/_page.mdx';
import BBoxDocumentAnnotationTab from './bbox_document_annotation_tab/_page.mdx';

# Annotations

In addition to the basic OCR functionality, Mistral Document AI API adds the `annotations` functionality, which allows you to extract information in a structured json-format that you provide.

<SectionTab as="h1" sectionId="before-you-start">Before You Start</SectionTab>

### What can you do with Annotations?

Specifically, it offers two types of annotations: 
- `bbox_annotation`: gives you the annotation of the bboxes extracted by the OCR model (charts/ figures etc) based on user requirement and provided bbox/image annotation format. The user may ask to describe/caption the figure for instance.
- `document_annotation`: returns the annotation of the entire document based on the provided document annotation format.


<div style={{ textAlign: 'center' }}>
  <img
    src="/img/ocr_annotations_explanation.png"
    alt="annotations_explanation_graph"
    width="600"
    style={{ borderRadius: '15px' }}
    className='mx-auto' 
  />
</div>

<SectionTab as="h2" variant="secondary" sectionId="key-capabilities">Key Capabilities</SectionTab>

* Labeling and annotating data
* Extraction and structuring of specific information from documents into a predefined JSON format
* Automation of data extraction to reduce manual entry and errors
* Efficient handling of large document volumes for enterprise-level applications

<SectionTab as="h2" variant="secondary" sectionId="common-use-cases">Common Use Cases</SectionTab>

* Parsing of forms, classification of documents, and processing of images, including text, charts, and signatures
* Conversion of charts to tables, extraction of fine print from figures, or definition of custom image types
* Capture of receipt data, including merchant names and transaction amounts, for expense management.
* Extraction of key information like vendor details and amounts from invoices for automated accounting.
* Extraction of key clauses and terms from contracts for easier review and management

<SectionTab as="h1" sectionId="how-it-works">How it Works</SectionTab>

<div style={{ textAlign: 'center' }}>
  <img
    src="/img/ocr_annotations_workflow.png"
    alt="annotations_workflow_graph"
    width="800"
    style={{ borderRadius: '15px' }}
    className='mx-auto' 
  />
</div>

<SectionTab as="h2" variant="secondary" sectionId="bbox-annotations-explanation">BBOX Annotations</SectionTab>

- All document types: 
  - After regular OCR is finished; we call a Vision capable LLM for all bboxes individually with the provided annotation format.

<SectionTab as="h2" variant="secondary" sectionId="document-annotation-explanation">Document Annotation</SectionTab>

- pdf/image: 
  - Independent of OCR; we convert all pages into images and send all images to a Vision capable LLM along with the provided annotation format.
- pptx/docx/...:
  - We run OCR first and send the output text markdown to a Vision capable LLM along with the provided annotation format.

<SectionTab as="h2" variant="secondary" sectionId="accepted-formats">Accepted Formats</SectionTab>

You can use our API with the following document formats:
- [OCR with pdf](basic_ocr#ocr-with-pdf)
- [OCR with image](basic_ocr#ocr-with-image): even from low-quality or handwritten sources.
- scans, DOCX, PPTX...

In the code snippets below, we will consider the `OCR with pdf` format.

<SectionTab as="h1" sectionId="usage">Usage</SectionTab>

### How to Annotate

As previously mentionned, you can either:
- Use the `bbox_annotation` functionality, allowing you to extract information from the bboxes of the document.
- Use the `document_annotation` functionality, allowing you to extract information from the entire document.
- Use both functionalities at the same time.

<ExplorerTabs id="usage">
  <ExplorerTab value="bbox-annotation" label="BBox Annotation">
    <BBoxAnnotationTab/>
  </ExplorerTab>
  <ExplorerTab value="document-annotation" label="Document Annotation">
     <DocumentAnnotationTab/>
  </ExplorerTab>
  <ExplorerTab value="bbox-document-annotation" label="BBox and Document Annotation">
    <BBoxDocumentAnnotationTab/>
  </ExplorerTab>
</ExplorerTabs>

<SectionTab as="h1" sectionId="cookbooks">Cookbooks</SectionTab>

For more information and guides on how to make use of OCR, we have the following cookbooks:
- [Data Extraction with Structured Outputs](https://colab.research.google.com/github/mistralai/cookbook/blob/main/mistral/ocr/data_extraction.ipynb)

<SectionTab as="h1" sectionId="faq">FAQ</SectionTab>

<Faq>
<FaqItem question="Are there any limits regarding the Document AI API?">
Yes, there are certain limitations for the Document Intelligence API. Uploaded document files must not exceed 50 MB in size and should be no longer than 1,000 pages.
</FaqItem>
<FaqItem question="Are there any limits regarding the Annotations?">
When using Document Annotations, the file cannot have more than 8 pages. BBox Annotations does not have the same limit.
</FaqItem>
</Faq>
