---
id: platform_overview
title: La Plateforme
slug: overview
---

[platform_url]: https://console.mistral.ai/
[deployment_img]: /img/deployment.png
[deployment_url]: https://console.mistral.ai/

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Mistral AI currently provides three types of access to Large Language Models: 
- **La Plateforme**: We provide API endpoints through [La Plateforme][platform_url] providing pay-as-you-go access to our latest models.
- **Cloud**: You can access Mistral AI models via your preferred [cloud platforms](/deployment/cloud/overview/).
- **Self-deployment**: You can self-deploy our open-weights models on your own on-premise infrastructure. Our open weights models are available under the [Apache 2.0](https://github.com/apache/.github/blob/main/LICENSE) License, available on [Hugging Face](https://huggingface.co/mistralai) or directly from [the documentation](/getting-started/models/weights).

[![deployment_img]][deployment_url]

### API Access with the La Plateforme

You will need to activate payments on your account to enable your API keys in the [La Plateforme][platform_url]. Check out the [Quickstart](/getting-started/quickstart/) guide to get started with your first Mistral API request. 

Explore the capabilities of our models:
- [Completion](/capabilities/completion)
- [Embeddings](/capabilities/embeddings/overview)
- [Function calling](/capabilities/function_calling)
- [JSON mode](/capabilities/structured-output/json_mode)
- [Guardrailing](/capabilities/guardrailing)


### Cloud-based deployments

For a comprehensive list of options to deploy and consume Mistral AI models on the cloud, head on to the **[cloud deployment section](/deployment/cloud/overview)**.

### Raw model weights

Raw model weights can be used in several ways: 
- For self-deployment, on cloud or on premise, using either [TensorRT-LLM](/deployment/self-deployment/trt) or [vLLM](/deployment/self-deployment/vllm), head on to **[Deployment](/deployment/self-deployment/skypilot)**
- For research, head-on to our [reference implementation repository](https://github.com/mistralai/mistral-src),
- For local deployment on consumer grade hardware, check out the [llama.cpp](https://github.com/ggerganov/llama.cpp) project or [Ollama](https://ollama.ai/).

