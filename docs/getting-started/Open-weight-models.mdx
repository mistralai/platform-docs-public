---
id: open_weight_models
title: Open-weight models
sidebar_position: 1.4
---

We open-source both pre-trained models and fine-tuned models. These models are not tuned for safety as we want to empower users to test and refine moderation based on their use cases. For safer models, follow our [guardrailing tutorial](/capabilities/guardrailing).

| Model               | Available Open-weight|Available via API| Description | Max Tokens| API Endpoints|
|--------------------|:--------------------:|:--------------------:|:--------------------:|:--------------------:|:--------------------:|
| Mistral 7B    | :heavy_check_mark: <br/> Apache2 |:heavy_check_mark: |The first dense model released by Mistral AI, perfect for experimentation, customization, and quick iteration. At the time of the release, it matched the capabilities of models up to 30B parameters. Learn more on our [blog post](https://mistral.ai/news/announcing-mistral-7b/)| 32k | `open-mistral-7b`|
| Mixtral 8x7B  |:heavy_check_mark: <br/> Apache2 | :heavy_check_mark: |A sparse mixture of experts model. As such, it leverages up to 45B parameters but only uses about 12B during inference, leading to better inference throughput at the cost of more vRAM. Learn more on the dedicated [blog post](https://mistral.ai/news/mixtral-of-experts/)| 32k  | `open-mixtral-8x7b`| 
| Mixtral 8x22B  |:heavy_check_mark: <br/> Apache2 | :heavy_check_mark: |A bigger sparse mixture of experts model. As such, it leverages up to 141B parameters but only uses about 39B during inference, leading to better inference throughput at the cost of more vRAM. Learn more on the dedicated [blog post](https://mistral.ai/news/mixtral-8x22b/)| 64k  | `open-mixtral-8x22b`| 
| Codestral  |:heavy_check_mark: <br/> MNPL|:heavy_check_mark: | A cutting-edge generative model that has been specifically designed and optimized for code generation tasks, including fill-in-the-middle and code completion | 32k  | `codestral-latest`| 
| Codestral Mamba | :heavy_check_mark: <br/> Apache2 | :heavy_check_mark: | A Mamba 2 language model specialized in code generation. Learn more on our [blog post](https://mistral.ai/news/codestral-mamba/) | 256k  | `open-codestral-mamba`| 
| Mathstral | :heavy_check_mark: <br/> Apache2 |  | A math-specific 7B model designed for math reasoning and scientific tasks. Learn more on our [blog post](https://mistral.ai/news/mathstral/) | 32k  | NA| 
| Mistral NeMo | :heavy_check_mark: <br/> Apache2 | :heavy_check_mark: | A 12B model built with the partnership with Nvidia. It is easy to use and a drop-in replacement in any system using Mistral 7B that it supersedes. Learn more on our [blog post](https://mistral.ai/news/mistral-nemo/) | 128k  | `open-mistral-nemo-latest`| 

## License
- Mistral 7B, Mixtral 8x7B, Mixtral 8x22B, Codestral Mamba, Mathstral, and Mistral NeMo are under [Apache 2 License](https://choosealicense.com/licenses/apache-2.0/), which permits their use without any constraints.
- Codestral is under [Mistral AI Non-Production (MNPL) License](https://mistral.ai/licences/MNPL-0.1.md).


## Downloading

| Model               |Download links|Features|
|--------------------|:--------------------|:--------------------|
| Mistral-7B-v0.1  | [Hugging Face](https://huggingface.co/mistralai/Mistral-7B-v0.1) <br/> [raw_weights](https://models.mistralcdn.com/mistral-7b-v0-1/mistral-7B-v0.1.tar) (md5sum: `37dab53973db2d56b2da0a033a15307f`) |- 32k vocabulary size <br/> - Rope Theta = 1e4 <br/> - With sliding window|
| Mistral-7B-Instruct-v0.2  | [Hugging Face](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) <br/> [raw_weights](https://models.mistralcdn.com/mistral-7b-v0-2/Mistral-7B-v0.2-Instruct.tar) (md5sum: `fbae55bc038f12f010b4251326e73d39`) | - 32k vocabulary size <br/> - Rope Theta = 1e6 <br/> - No sliding window |
| Mistral-7B-v0.3  | [Hugging Face](https://huggingface.co/mistralai/Mistral-7B-v0.3) <br/> [raw_weights](https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar) (md5sum: `0663b293810d7571dad25dae2f2a5806`) |- Extended vocabulary to 32768 <br/> |
| Mistral-7B-Instruct-v0.3  | [Hugging Face](https://huggingface.co/mistralai/Mistral-7B-v0.3) <br/> [raw_weights](https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar) (md5sum: `80b71fcb6416085bcb4efad86dfb4d52`) |- Extended vocabulary to 32768 <br/> - Supports v3 Tokenizer <br/> - Supports function calling|
| Mixtral-8x7B-v0.1   | [Hugging Face](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) |- 32k vocabulary size <br/> - Rope Theta = 1e6|
| Mixtral-8x7B-Instruct-v0.1  | [Hugging Face](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) <br/> [raw_weights](https://models.mistralcdn.com/mixtral-8x7b-v0-1/Mixtral-8x7B-v0.1-Instruct.tar) (md5sum: `8e2d3930145dc43d3084396f49d38a3f`) |- 32k vocabulary size <br/> - Rope Theta = 1e6|
| Mixtral-8x7B-v0.3  | Updated model coming soon!  |- Extended vocabulary to 32768 <br/> - Supports v3 Tokenizer |
| Mixtral-8x7B-Instruct-v0.3  |Updated model coming soon!   |- Extended vocabulary to 32768 <br/> - Supports v3 Tokenizer <br/> - Supports function calling|
| Mixtral-8x22B-v0.1  | [Hugging Face](https://huggingface.co/mistralai/Mixtral-8x22B-v0.1) <br/> [raw_weights](magnet:?xt=urn:btih:9238b09245d0d8cd915be09927769d5f7584c1c9&dn=mixtral-8x22b&tr=udp%3A%2F%http://2Fopen.demonii.com%3A1337%2Fannounce&tr=http%3A%2F%https://t.co/OdtBUsbeV5%3A1337%2Fannounce) (md5sum: `0535902c85ddbb04d4bebbf4371c6341`) |- 32k vocabulary size |
| Mixtral-8x22B-Instruct-v0.1/ <br/> Mixtral-8x22B-Instruct-v0.3 | [Hugging Face](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1) <br/> [raw_weights](https://models.mistralcdn.com/mixtral-8x22b-v0-3/mixtral-8x22B-Instruct-v0.3.tar) (md5sum: `471a02a6902706a2f1e44a693813855b`)|- 32768 vocabulary size |
| Mixtral-8x22B-v0.3  | [raw_weights](https://models.mistralcdn.com/mixtral-8x22b-v0-3/mixtral-8x22B-v0.3.tar) (md5sum: `a2fa75117174f87d1197e3a4eb50371a`) | - 32768 vocabulary size <br/> - Supports v3 Tokenizer |
| Codestral-22B-v0.1  | [Hugging Face](https://huggingface.co/mistralai/Codestral-22B-v0.1) <br/> [raw_weights](https://models.mistralcdn.com/codestral-22b-v0-1/codestral-22B-v0.1.tar) (md5sum: `1ea95d474a1d374b1d1b20a8e0159de3`) | - 32768 vocabulary size <br/> - Supports v3 Tokenizer |
| Codestral-Mamba-7B-v0.1  | [Hugging Face](https://huggingface.co/mistralai/mamba-codestral-7B-v0.1) <br/> [raw_weights](https://models.mistralcdn.com/codestral-mamba-7b-v0-1/codestral-mamba-7B-v0.1.tar)(md5sum: `d3993e4024d1395910c55db0d11db163`) | - 32768 vocabulary size <br/> - Supports v3 Tokenizer |
| Mathstral-7B-v0.1  | [Hugging Face](https://huggingface.co/mistralai/mathstral-7B-v0.1) <br/> [raw_weights](https://models.mistralcdn.com/mathstral-7b-v0-1/mathstral-7B-v0.1.tar)(md5sum: `5f05443e94489c261462794b1016f10b`) | - 32768 vocabulary size <br/> - Supports v3 Tokenizer |
| Mistral-NeMo-Base-2407  | [Hugging Face](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407) <br/> [raw_weights](https://models.mistralcdn.com/mistral-nemo-2407/mistral-nemo-base-2407.tar)(md5sum: `c5d079ac4b55fc1ae35f51f0a3c0eb83`) | - 131k vocabulary size <br/> - Supports tekken.json tokenizer |  
| Mistral-NeMo-Instruct-2407  | [Hugging Face](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407) <br/> [raw_weights](https://models.mistralcdn.com/mistral-nemo-2407/mistral-nemo-instruct-2407.tar)(md5sum: `296fbdf911cb88e6f0be74cd04827fe7`) | - 131k vocabulary size <br/> - Supports tekken.json tokenizer <br/> - Supports function calling |


## Sizes

| Name               | Number of parameters | Number of active parameters | Min. GPU RAM for inference (GB) |
|--------------------|:--------------------:|:---------------------------:|:-------------------------------:|
| Mistral-7B-v0.3    | 7.3B                 | 7.3B                        | 16                              |
| Mixtral-8x7B-v0.1  | 46.7B                  | 12.9B                         | 100                             |
| Mixtral-8x22B-v0.3  | 140.6B                  | 39.1B                         | 300                             |
| Codestral-22B-v0.1  | 22.2B | 22.2B | 60 |
| Codestral-Mamba-7B-v0.1  | 7.3B | 7.3B | 16 |
| Mathstral-7B-v0.1  | 7.3B | 7.3B | 16 |
| Mistral-NeMo-12B-v0.1  | 12B | 12B | 28 - bf16 <br/> 16 - fp8 |
| Mistral-NeMo-12B-v0.1  | 12B | 12B | 28 - bf16 <br/> 16 - fp8 |

## How to run? 
Check out [mistral-inference](https://github.com/mistralai/mistral-inference/), a Python package for running our models. You can install `mistral-inference` by
```
pip install mistral-inference
``` 

To learn more about how to use mistral-inference, take a look at the [README](https://github.com/mistralai/mistral-inference/blob/main/README.md) and dive into this colab notebook to get started:

<a target="_blank" href="https://colab.research.google.com/github/mistralai/mistral-inference/blob/main/tutorials/getting_started.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

