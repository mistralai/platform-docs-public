---
id: compare-models
title: Compare Models
table_of_contents: false
---

import { SectionTab } from '@/components/layout/section-tab';
import { NuqsAdapter } from 'nuqs/adapters/next/app';
import { ModelsSelector } from './_components/models-selector';
import { Suspense } from 'react';

<Suspense>
  <NuqsAdapter>
    <ModelsSelector />
  </NuqsAdapter>
</Suspense>

<SectionTab sectionId="external-benchmarks">Benchmarks</SectionTab>

For more **quantitative** comparisons between our models, we recommend visiting our blog posts. During model releases, we may directly report **quantitative** public and private benchmarks for our models and competitive models.

You can also explore third-party performance metrics, such as:

| Benchmark Name       | Description                                                                                      | Link                                               |
| -------------------- | ------------------------------------------------------------------------------------------------ | -------------------------------------------------- |
| Artificial Analysis  | Compares AI models across quality, price, output speed, latency, context window, and more.       | [Visit](https://artificialanalysis.ai/leaderboard) |
| LMArena Arena        | Human-preference benchmark evaluating model output quality through direct comparisons.           | [Visit](https://lmarena.ai/leaderboard)            |
| Scale AI Leaderboard | Reports public and private benchmarks in coding, instruction following, math, and other domains. | [Visit](https://scale.com/leaderboard)             |
| OpenRouter Rankings  | Ranks AI models based on general usage and popularity across different use cases and tasks.      | [Visit](https://openrouter.ai/rankings)            |
| CTO Bench            | Evaluates AI models on real end-to-end coding tasks.                                             | [Visit](https://cto.new/bench)                     |
