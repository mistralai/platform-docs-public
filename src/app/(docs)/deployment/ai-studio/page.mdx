import { SectionTab } from '@/components/layout/section-tab';

[platform_url]: https://console.mistral.ai/
[deployment_img]: /img/deployment.png
[deployment_url]: https://console.mistral.ai/

import { Tabs, TabItem } from '@/components/common/multi-codeblock';

# AI Studio

[Mistral AI Studio][platform_url] is a platform where you can access and manage models, usage, APIs, organizations, workspaces, and a variety of other features.  
We offer flexible access to our models through a range of options, services, and customizable solutions-including playgrounds, fine-tuning, self-hosting and more-to meet your specific needs.

<Image
  url={['/img/deployment_overview.png', '/img/deployment_overview_dark.png']}
  alt="deployment_overview"
  width="900px"
  centered
/>

**AI Studio** (previously "La Plateforme") provides API endpoints for pay-as-you-go access to our latest models. It allows you to manage workspaces, usage, and a variety of other features—a full stack with multiple options for diverse enterprise use.
We offer three main solutions:
  - **Serverless**: Public API endpoints managed by the Mistral team through [AI Studio][platform_url].
  - **Dedicated Serverless**: Dedicated instances tailored to your enterprise needs through a dedicated AI Studio. [Reach out](https://mistral.ai/contact) to us if you're interested!
  - **Self-Hosted**: Dedicated on-premise instances for your enterprise needs through a self-hosted AI Studio. [Reach out](https://mistral.ai/contact) to us if you're interested!

Below, you can find a non-exhaustive table with further details regarding each offering.
| Feature                   | AI Studio - Serverless                     | AI Studio - Serverless Dedicated                         | AI Studio - Self-Hosted (VPC/on-prem) |
|---------------------------|--------------------------------------------|----------------------------------------------------------|---------------------------------------|
| **Infrastructure**        | Owned & Managed by Mistral                 | Owned & Managed by Mistral                               | Owned & Managed by Client             |
| **Data Access & Storage** | Shared data plane, multi-tenant storage    | Virtually isolated per Client                            | Hosted by Client                      |
| **Network**               | Public Internet                            | Private Subnet                                           | Hosted by Client                      |
| **Inference Compute**     | Shared                                     | Shared - With possibility for dedicated GPU options      | Hosted by Client                      |
:::note
Some features may be model-dependent and vary, contact our team for further details.
:::

<SectionTab as="h2" variant="secondary" sectionId="other-options">Other Options</SectionTab>

There are also other ways of accessing and using our models for your needs:
- **Third Party Cloud**: You can access Mistral AI models via your preferred [cloud platforms](/deployment/cloud).
- **Open Weights**: You can self-deploy our models on your own on-premise infrastructure.
    - **Self-Deploy on your Own**: You can self-deploy our open models on your own. Multiple open-weights models are available under the [Apache 2.0](https://github.com/apache/.github/blob/main/LICENSE) License, you can find them on [Hugging Face](https://huggingface.co/mistralai).
    - **Self-Deploy with Enterprise Support**: You can also self-deploy our models, both open and frontier, with enterprise support. Reach out to us [here](https://mistral.ai/contact) if you’re interested!

<SectionTab as="h1" sectionId="api-access-with-ai-studio">API Access with AI Studio - Serverless</SectionTab>

You will need to activate payments on your account to enable your API keys in the [AI Studio][platform_url]. Check out the [Quickstart](/getting-started/quickstart/) guide to get started with your first Mistral API request. 

Explore diverse capabilities of our models:
- [Completion](/capabilities/completion)
- [Embeddings](/capabilities/embeddings/overview)
- [Function calling](/capabilities/function_calling)
- [JSON mode](/capabilities/structured-output/json_mode)
- [Guardrailing](/capabilities/guardrailing)
- Much more...

<SectionTab as="h1" sectionId="cloud-based-deployments">Cloud-based deployments</SectionTab>

For a comprehensive list of options to deploy and consume Mistral AI models on the cloud, head on to the **[cloud deployment section](/deployment/cloud)**.

<SectionTab as="h1" sectionId="raw-model-weights">Raw model weights</SectionTab>

Raw model weights can be used in several ways: 
- For self-deployment, on cloud or on premise, using either [TensorRT-LLM](/deployment/self-deployment/trt) or [vLLM](/deployment/self-deployment/vllm), head on to **[Deployment](/deployment/self-deployment/skypilot)**
- For research, head-on to our [reference implementation repository](https://github.com/mistralai/mistral-inference),
- For local deployment on consumer grade hardware, check out the [llama.cpp](https://github.com/ggerganov/llama.cpp) project or [Ollama](https://ollama.ai/).
