---
id: platform_overview
title: La Plateforme
slug: overview
---

[platform_url]: https://console.mistral.ai/
[deployment_img]: /static/img/deployment.png
[deployment_url]: https://console.mistral.ai/

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Mistral AI currently provides three types of access to Large Language Models: 
- **La Plateforme**: We provide API endpoints through [La Plateforme][platform_url] providing pay-as-you-go access to our latest models.
- **Cloud**: You can access Mistral AI models via your preferred [cloud platforms](/docs//deployment/cloud/overview.mdx).
- **Self-deployment**: You can self-deploy our open-weights models on your own on-premise infrastructure. Our open weights models are available under the [Apache 2.0](https://github.com/apache/.github/blob/main/LICENSE) License, available on [Hugging Face](https://huggingface.co/mistralai) or directly from [the documentation](/docs/getting-started/models/weights.md).

[![deployment_img]][deployment_url]

### API Access with the La Plateforme

You will need to activate payments on your account to enable your API keys in the [La Plateforme][platform_url]. Check out the [Quickstart](/getting-started/quickstart/) guide to get started with your first Mistral API request. 

Explore the capabilities of our models:
- [Completion](/docs/capabilities/completion.mdx)
- [Embeddings](/docs/capabilities/embeddings.mdx)
- [Function calling](/docs/capabilities/function-calling.mdx)
- [JSON mode](/docs/capabilities/structured-output/json-mode.mdx)
- [Guardrailing](/docs/capabilities/guardrailing.mdx)


### Cloud-based deployments

For a comprehensive list of options to deploy and consume Mistral AI models on the cloud, head on to the **[cloud deployment section](/docs/deployment/cloud/overview.mdx)**.

### Raw model weights

Raw model weights can be used in several ways: 
- For self-deployment, on cloud or on premise, using either [TensorRT-LLM](/docs/deployment/self-deployment/trt.mdx) or [vLLM](/docs/deployment/self-deployment/vllm.mdx), head on to **[Deployment](/docs/deployment/self-deployment/skypilot.mdx)**
- For research, head-on to our [reference implementation repository](https://github.com/mistralai/mistral-src),
- For local deployment on consumer grade hardware, check out the [llama.cpp](https://github.com/ggerganov/llama.cpp) project or [Ollama](https://ollama.ai/).

