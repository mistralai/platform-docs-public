---
id: self_deployment_overview
title: Self-Deployment
slug: overview
---

# Self-Deployment

Mistral AI models can be **self-deployed on your own infrastructure** through various
inference engines. We recommend using [vLLM](https://vllm.readthedocs.io/), a
highly-optimized Python-only serving framework which can expose an OpenAI-compatible
API.

Other inference engine alternatives include
[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) and
[TGI](https://huggingface.co/docs/text-generation-inference/index).

You can also leverage specific tools to facilitate infrastructure management, such as
[SkyPilot](https://skypilot.readthedocs.io) or [Cerebrium](https://www.cerebrium.ai).

:::tip
For full-stack enterprise self-deployment - from efficient model inference to team management - we recommend [reaching out to us](https://mistral.ai/contact) for a **self-hosted AI Studio**.
:::